# Object_Detection_Classification_Captioning

1. Using "Image classification", predict which class(es) (i.e. items) belong to it. Out is 'score and 'Label", 'Box', 
2. Using 'Visual question answering (VQA)", ask a question, relevant to the image and get the answer
3. Using 'CLIP', get a higher score for a class label, which is more relevant
4. Using 'VisionEncoderDecoderModel ', get reasonable image captioning results
5. YOLO

**Reference links:**
IMP: https://huggingface.co/google/vit-base-patch16-224
IMP: https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForImageClassification
IMP: https://huggingface.co/facebook/convnext-tiny-224
IMP: https://huggingface.co/facebook/detr-resnet-50
IMP: https://huggingface.co/dandelin/vilt-b32-finetuned-vqa
IMP: https://huggingface.co/openai/clip-vit-base-patch32

https://arxiv.org/abs/2010.11929
https://www.cvlib.net/

https://www.youtube.com/watch?v=oL-xmufhZM8

https://www.youtube.com/watch?v=Az7aYcZ-oJE

https://github.com/NielsRogge/Transformers-Tutorials/blob/master/HuggingFace_vision_ecosystem_overview_(June_2022).ipynb
https://huggingface.co/docs/transformers/model_doc/vit
https://huggingface.co/docs/transformers/main_classes/pipelines
https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForObjectDetection
https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForImageSegmentation
https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForVision2Seq
https://huggingface.co/docs/transformers/main/en/model_doc/clip
